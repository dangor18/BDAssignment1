% Created 2024-02-28 Wed 18:55
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\hypersetup{colorlinks=true, allcolors=blue} \usepackage{titlesec} \usepackage{geometry} \geometry{margin=1.5in}
\author{Matthew}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Matthew},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 28.3 (Org mode 9.7)}, 
 pdflang={English}}
\begin{document}

\begin{LATEX}
\begin{titlepage}

  \centering
  {\Large\bfseries Big Data Assignment 1 \par}
  \vspace{0.5cm}

  {\large KFWJOR001 MRCGAB004 WHLJOS001 CRGMAT002\par}
  \vspace{0.5cm}

  {\large March 1, 2024\par}

  \vspace{2cm}
  \includegraphics[width=0.5\textwidth]{uct.png}
\end{titlepage}
\tableofcontents
\end{LATEX}
\pagebreak
\section{Find or Create a Suitable Data Set}
\label{sec:org297bd33}
\subsection{Explanation of data set}
\label{sec:org8e047ca}
Link to the dataset: \url{https://github.com/zygmuntz/goodbooks-10k}\\
\linebreak
The dataset initially contained multiple csv files representing information on books, and user data on book ratings. This dataset was chosen as its ideal for a MongoDB database due to its semi-structured nature and nested data, which is particularly useful for storing ratings and book tags. \\
\linebreak
\textbf{Dataset Content}:
\begin{itemize}
\item \textbf{books.csv}: Each entry represents a book with a unique \texttt{book\_id}. There are multiple data fields for a book:
\begin{itemize}
\item \textbf{\texttt{book\_id}, \texttt{goodreads\_book\_id}, \texttt{best\_book\_id}, \texttt{work\_id}}: Unique id’s representing a book, each with a different purpose. We only used \texttt{book\_id} and \texttt{goodreads\_book\_id} as they’re used to link books to user \texttt{ratings} and user \texttt{to\_read} lists.
\item \textbf{\texttt{ratings\_1}, \texttt{ratings\_2}, …}: Number of user ratings by rating value. eg. \texttt{ratings\_1} represents the number of 1 star ratings given to that book.
\item The rest of the fields are self explanatory but include info relating to authors, title, release date, and isbn number.
\end{itemize}
\item \textbf{ratings.csv}: Each entry is a \texttt{user\_id} to \texttt{book\_id} mapping with a rating.
\begin{itemize}
\item \textbf{\texttt{book\_tags.csv}}: Each entry is a \texttt{book\_id} to \texttt{tag\_id} mapping.
\item \textbf{\texttt{tags.csv}}: Each entry is a tag\textsubscript{id} to tag\textsubscript{name} mapping.
\item \textbf{\texttt{to\_read.csv}} : Each entry is a \texttt{user\_id} to \texttt{goodreads\_book\_id} mapping which represents a user adding a book to their \texttt{to\_read} list.
\end{itemize}
\end{itemize}
\subsection{Data Pre-Processing:}
\label{sec:org5ec2fc4}
The data was processed such that the data was represented in JSON format with evidence of nested objects so that we could demonstrate the capabilities of MongoDB\\
\linebreak
Here is a quick outline on how we processed the data to create JSON files:\\
Libraries used: \texttt{Pandas}, \texttt{PyArrow}, \texttt{Faker}\\
\linebreak
\texttt{Pandas} was used to load the csv files into dataframes where we merged data and applied \texttt{group by} aggregate functions to obtain lists of data objects per a unique entry id. This was useful, for example, when we obtained a list of tags per \texttt{book\_id}.\\
\linebreak
\texttt{Faker} was used to generate random usernames for each id that were then written to \texttt{user\_data.csv}.
The \texttt{dataframes} were then converted into JSON files.\\
\linebreak
All data pre-processing code is in the data-processing directory but the output JSON files are included in the final submission.
**
\end{document}
